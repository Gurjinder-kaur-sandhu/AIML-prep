# Decision Tree Classification ‚Äì Breast Cancer Dataset

## üìå Project Overview

This project demonstrates how to build, train, and evaluate a **Decision Tree Classifier** using the **Breast Cancer dataset** from `scikit-learn`.
The goal is to understand the **working of Decision Trees**, handle **overfitting**, and evaluate model performance using standard metrics.

---

## üéØ Objectives

* Understand the intuition behind Decision Trees
* Implement a Decision Tree Classifier step by step
* Analyze **overfitting** using different tree depths
* Evaluate the model using **accuracy** and **confusion matrix**

---

## üìÇ Dataset

* **Name:** Breast Cancer Wisconsin Dataset
* **Source:** `sklearn.datasets.load_breast_cancer`
* **Classes:**

  * `0` ‚Üí Malignant (cancer)
  * `1` ‚Üí Benign (non-cancer)

---

## üõ†Ô∏è Technologies Used

* Python
* scikit-learn
* NumPy

---

## üîÅ Machine Learning Workflow

### 1Ô∏è‚É£ Load the Dataset

The Breast Cancer dataset is loaded using `load_breast_cancer()` from sklearn.

### 2Ô∏è‚É£ Feature and Target Selection

* **X** ‚Üí Input features (tumor measurements)
* **y** ‚Üí Target labels (malignant or benign)

### 3Ô∏è‚É£ Train-Test Split

The dataset is split into:

* 80% training data
* 20% testing data

This helps evaluate model performance on unseen data.

### 4Ô∏è‚É£ Model Building

A **DecisionTreeClassifier** is used with:

* `criterion = "gini"`
* Different values of `max_depth` to control overfitting

### 5Ô∏è‚É£ Model Training

The model is trained using the `.fit()` method on training data.

### 6Ô∏è‚É£ Prediction

Predictions are made on test data using `.predict()`.

### 7Ô∏è‚É£ Evaluation

Model performance is evaluated using:

* **Accuracy Score**
* **Confusion Matrix**

---

## üå≥ Overfitting Analysis

Decision Trees can easily overfit if they grow too deep.

### Experiments Performed:

* `max_depth = 2`

  * Model is very simple
  * May underfit
* `max_depth = 5`

  * Balanced complexity
  * Better generalization

### Key Observation:

> **Limiting tree depth helps reduce overfitting.**

The best depth is chosen based on **test accuracy**, not training accuracy.

---

## üìä Evaluation Metrics

* **Accuracy Score:** Measures overall correctness
* **Confusion Matrix:** Shows detailed classification results

  * True Positives
  * True Negatives
  * False Positives
  * False Negatives

This is especially important for medical datasets.

---

## üß† Key Learnings

* Decision Trees split data based on feature conditions
* Gini Index measures impurity
* Deep trees can overfit
* Controlling `max_depth` improves generalization
* Confusion matrix provides better insight than accuracy alone

---

## ‚úçÔ∏è Conclusion

This project provides a clear understanding of how Decision Trees work, how overfitting occurs, and how to control it using tree depth. The Breast Cancer dataset makes this a practical and meaningful classification problem.

---

## üöÄ Future Improvements

* Tree visualization
* Hyperparameter tuning (`min_samples_split`, `min_samples_leaf`)
* Precision, Recall, and F1-score analysis
* Cross-validation

---

**Author:** Guri
**Purpose:** Machine Learning Fundamentals & Interview Preparation
